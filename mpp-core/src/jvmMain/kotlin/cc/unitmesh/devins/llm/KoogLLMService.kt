package cc.unitmesh.devins.llm

import ai.koog.agents.core.agent.AIAgent
import ai.koog.prompt.dsl.prompt
import ai.koog.prompt.executor.clients.anthropic.AnthropicModels
import ai.koog.prompt.executor.clients.deepseek.DeepSeekLLMClient
import ai.koog.prompt.executor.clients.deepseek.DeepSeekModels
import ai.koog.prompt.executor.clients.google.GoogleModels
import ai.koog.prompt.executor.clients.list
import ai.koog.prompt.executor.clients.openai.OpenAIModels
import ai.koog.prompt.executor.clients.openrouter.OpenRouterModels
import ai.koog.prompt.executor.llms.SingleLLMPromptExecutor
import ai.koog.prompt.executor.llms.all.*
import ai.koog.prompt.llm.LLMCapability
import ai.koog.prompt.llm.LLMProvider
import ai.koog.prompt.llm.LLModel
import ai.koog.prompt.params.LLMParams
import ai.koog.prompt.streaming.StreamFrame
import cc.unitmesh.devins.compiler.DevInsCompilerFacade
import cc.unitmesh.devins.compiler.context.CompilerContext
import cc.unitmesh.devins.filesystem.EmptyFileSystem
import cc.unitmesh.devins.filesystem.ProjectFileSystem
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.cancellable
import kotlinx.coroutines.flow.flow

class KoogLLMService(private val config: ModelConfig) {
    /**
     * æµå¼å‘é€æç¤ºï¼Œæ”¯æŒ DevIns ç¼–è¯‘ã€SpecKit å‘½ä»¤å’Œå¤šè½®å¯¹è¯
     * @param userPrompt ç”¨æˆ·è¾“å…¥çš„æç¤ºæ–‡æœ¬ï¼ˆå¯ä»¥åŒ…å« DevIns è¯­æ³•å’Œå‘½ä»¤ï¼‰
     * @param fileSystem é¡¹ç›®æ–‡ä»¶ç³»ç»Ÿï¼Œç”¨äºæ”¯æŒ SpecKit ç­‰å‘½ä»¤ï¼ˆå¯é€‰ï¼‰
     * @param historyMessages å†å²æ¶ˆæ¯åˆ—è¡¨ï¼Œç”¨äºå¤šè½®å¯¹è¯ï¼ˆå¯é€‰ï¼‰
     */
    fun streamPrompt(
        userPrompt: String, 
        fileSystem: ProjectFileSystem = EmptyFileSystem(),
        historyMessages: List<Message> = emptyList()
    ): Flow<String> = flow {
        val executor = createExecutor()
        val model = getModelForProvider()

        // åˆ›å»ºå¸¦æœ‰æ–‡ä»¶ç³»ç»Ÿçš„ç¼–è¯‘ä¸Šä¸‹æ–‡
        val context = CompilerContext().apply {
            this.fileSystem = fileSystem
        }

        // ç¼–è¯‘ DevIns ä»£ç ï¼Œæ”¯æŒ SpecKit å‘½ä»¤ï¼ˆåªç¼–è¯‘æœ€æ–°çš„ç”¨æˆ·è¾“å…¥ï¼‰
        println("ğŸ” [KoogLLMService] å¼€å§‹ç¼–è¯‘ DevIns ä»£ç ...")
        println("ğŸ” [KoogLLMService] ç”¨æˆ·è¾“å…¥: $userPrompt")
        println("ğŸ” [KoogLLMService] å†å²æ¶ˆæ¯æ•°: ${historyMessages.size}")
        println("ğŸ” [KoogLLMService] æ–‡ä»¶ç³»ç»Ÿ: ${fileSystem.javaClass.simpleName}")
        println("ğŸ” [KoogLLMService] é¡¹ç›®è·¯å¾„: ${fileSystem.getProjectPath()}")

        val compiledResult = DevInsCompilerFacade.compile(userPrompt, context)
        val finalPrompt = compiledResult.output

        println("ğŸ” [KoogLLMService] ç¼–è¯‘å®Œæˆ!")
        println("ğŸ” [KoogLLMService] ç¼–è¯‘ç»“æœ: ${if (compiledResult.isSuccess()) "æˆåŠŸ" else "å¤±è´¥"}")
        println("ğŸ” [KoogLLMService] å‘½ä»¤æ•°é‡: ${compiledResult.statistics.commandCount}")
        println("ğŸ” [KoogLLMService] ç¼–è¯‘è¾“å‡º: $finalPrompt")
        if (compiledResult.hasError) {
            println("âš ï¸ [KoogLLMService] ç¼–è¯‘é”™è¯¯: ${compiledResult.errorMessage}")
        }
        
        // æ„å»ºåŒ…å«å†å²çš„ prompt
        val prompt = prompt(
            id = "chat",
            params = LLMParams(temperature = config.temperature, toolChoice = LLMParams.ToolChoice.None)
        ) {
            // æ·»åŠ å†å²æ¶ˆæ¯
            historyMessages.forEach { message ->
                when (message.role) {
                    MessageRole.USER -> user(message.content)
                    MessageRole.ASSISTANT -> assistant(message.content)
                    MessageRole.SYSTEM -> system(message.content)
                }
            }
            
            // æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆç¼–è¯‘åçš„ï¼‰
            user(finalPrompt)
        }

        executor.executeStreaming(prompt, model)
            .cancellable()
            .collect { frame ->
                when (frame) {
                    is StreamFrame.Append -> {
                        emit(frame.text)
                    }
                    is StreamFrame.End -> {
                        // Stream ended successfully
                    }
                    is StreamFrame.ToolCall -> {
                        // Tool calls (å¯ä»¥åç»­æ‰©å±•)
                    }
                }
            }
    }

    suspend fun sendPrompt(prompt: String): String {
        return try {
            val executor = createExecutor()
            
            val agent = AIAgent(
                promptExecutor = executor,
                llmModel = getModelForProvider()
            )
            
            agent.run(prompt)
        } catch (e: Exception) {
            "[Error: ${e.message}]"
        }
    }

    private fun getModelForProvider(): LLModel {
        return when (config.provider) {
            LLMProviderType.OPENAI -> {
                OpenAIModels.list()
                    .find { it.id == config.modelName }
                    ?: createDefaultModel(LLMProvider.OpenAI, 128000)
            }
            LLMProviderType.DEEPSEEK -> {
                DeepSeekModels.list()
                    .find { it.id == config.modelName }
                    ?: createDefaultModel(LLMProvider.DeepSeek, 64000)
            }
            LLMProviderType.ANTHROPIC -> {
                AnthropicModels.list()
                    .find { it.id == config.modelName }
                    ?: createDefaultModel(LLMProvider.Anthropic, 200000)
            }
            LLMProviderType.GOOGLE -> {
                GoogleModels.list()
                    .find { it.id == config.modelName }
                    ?: createDefaultModel(LLMProvider.Google, 128000)
            }
            LLMProviderType.OPENROUTER -> {
                OpenRouterModels.list()
                    .find { it.id == config.modelName }
                    ?: createDefaultModel(LLMProvider.OpenRouter, 128000)
            }
            LLMProviderType.OLLAMA -> {
                LLModel(
                    provider = LLMProvider.Ollama,
                    id = config.modelName,
                    capabilities = listOf(LLMCapability.Completion, LLMCapability.Tools),
                    contextLength = 128000
                )
            }
            LLMProviderType.BEDROCK -> {
                createDefaultModel(LLMProvider.Bedrock, 128000)
            }
        }
    }

    private fun createDefaultModel(provider: LLMProvider, contextLength: Long): LLModel {
        return LLModel(
            provider = provider,
            id = config.modelName,
            capabilities = listOf(),
            contextLength = contextLength,
        )
    }


    /**
     * Create appropriate executor based on provider configuration
     */
    private fun createExecutor(): SingleLLMPromptExecutor {
        return when (config.provider) {
            LLMProviderType.OPENAI -> simpleOpenAIExecutor(config.apiKey)
            LLMProviderType.ANTHROPIC -> simpleAnthropicExecutor(config.apiKey)
            LLMProviderType.GOOGLE -> simpleGoogleAIExecutor(config.apiKey)
            LLMProviderType.DEEPSEEK -> {
                SingleLLMPromptExecutor(DeepSeekLLMClient(config.apiKey))
            }
            LLMProviderType.OLLAMA -> simpleOllamaAIExecutor(
                baseUrl = config.baseUrl.ifEmpty { "http://localhost:11434" }
            )
            LLMProviderType.OPENROUTER -> simpleOpenRouterExecutor(config.apiKey)
            LLMProviderType.BEDROCK -> {
                // Bedrock requires AWS credentials in format: accessKeyId:secretAccessKey
                val credentials = config.apiKey.split(":")
                if (credentials.size != 2) {
                    throw IllegalArgumentException("Bedrock requires API key in format: accessKeyId:secretAccessKey")
                }
                simpleBedrockExecutor(
                    awsAccessKeyId = credentials[0],
                    awsSecretAccessKey = credentials[1]
                )
            }
        }
    }

    suspend fun validateConfig(): Result<String> {
        return try {
            val response = sendPrompt("Say 'OK' if you can hear me.")
            Result.success(response)
        } catch (e: Exception) {
            Result.failure(e)
        }
    }

    companion object {
        fun create(config: ModelConfig): KoogLLMService {
            if (!config.isValid()) {
                throw IllegalArgumentException("Invalid model configuration: ${config.provider} requires ${if (config.provider == LLMProviderType.OLLAMA) "baseUrl and modelName" else "apiKey and modelName"}")
            }
            return KoogLLMService(config)
        }
    }
}
