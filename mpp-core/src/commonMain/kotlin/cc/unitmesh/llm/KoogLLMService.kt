package cc.unitmesh.llm

import ai.koog.agents.core.agent.AIAgent
import ai.koog.prompt.dsl.prompt
import ai.koog.prompt.executor.llms.SingleLLMPromptExecutor
import ai.koog.prompt.llm.LLModel
import ai.koog.prompt.params.LLMParams
import ai.koog.prompt.streaming.StreamFrame
import cc.unitmesh.agent.logging.getLogger
import cc.unitmesh.devins.compiler.service.DevInsCompilerService
import cc.unitmesh.devins.filesystem.EmptyFileSystem
import cc.unitmesh.devins.filesystem.ProjectFileSystem
import cc.unitmesh.devins.llm.Message
import cc.unitmesh.devins.llm.MessageRole
import cc.unitmesh.llm.compression.*
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.cancellable
import kotlinx.coroutines.flow.flow
import kotlinx.datetime.Clock

/**
 * LLM æœåŠ¡
 *
 * @param config æ¨¡å‹é…ç½®
 * @param compressionConfig å‹ç¼©é…ç½®
 * @param compilerService å¯é€‰çš„ç¼–è¯‘å™¨æœåŠ¡ï¼Œç”¨äºç¼–è¯‘ DevIns å‘½ä»¤
 *                        å¦‚æœä¸æä¾›ï¼Œå°†ä½¿ç”¨ DevInsCompilerService.getInstance()
 */
class KoogLLMService(
    private val config: ModelConfig,
    private val compressionConfig: CompressionConfig = CompressionConfig(),
    private val compilerService: DevInsCompilerService? = null
) {
    private val logger = getLogger("KoogLLMService")

    private val executor: SingleLLMPromptExecutor by lazy {
        ExecutorFactory.create(config)
    }

    private val model: LLModel by lazy {
        ModelRegistry.createModel(config.provider, config.modelName)
            ?: ModelRegistry.createGenericModel(config.provider, config.modelName)
    }

    private val compressionService: ChatCompressionService by lazy {
        ChatCompressionService(executor, model, compressionConfig)
    }

    // è·å–å®é™…ä½¿ç”¨çš„ç¼–è¯‘å™¨æœåŠ¡
    private val actualCompilerService: DevInsCompilerService
        get() = compilerService ?: DevInsCompilerService.getInstance()

    // Token è¿½è¸ª
    private var lastTokenInfo: TokenInfo = TokenInfo()
    private var messagesSinceLastCompression = 0
    private var hasFailedCompressionAttempt = false

    fun streamPrompt(
        userPrompt: String,
        fileSystem: ProjectFileSystem = EmptyFileSystem(),
        historyMessages: List<Message> = emptyList(),
        compileDevIns: Boolean = true,
        onTokenUpdate: ((TokenInfo) -> Unit)? = null,
        onCompressionNeeded: ((Int, Int) -> Unit)? = null
    ): Flow<String> = flow {
        val finalPrompt = if (compileDevIns) {
            compilePrompt(userPrompt, fileSystem)
        } else {
            userPrompt
        }

        val promptLength = finalPrompt.length
        logger.info { "ğŸš€ [LLM] Starting stream request - prompt length: $promptLength chars, model: ${config.modelName}" }
        val startTime = Clock.System.now().toEpochMilliseconds()

        val prompt = buildPrompt(finalPrompt, historyMessages)
        var chunkCount = 0
        var totalChars = 0

        try {
            executor.executeStreaming(prompt, model)
                .cancellable()
                .collect { frame ->
                    when (frame) {
                        is StreamFrame.Append -> {
                            chunkCount++
                            totalChars += frame.text.length
                            if (chunkCount == 1) {
                                val ttfb = Clock.System.now().toEpochMilliseconds() - startTime
                                logger.info { "ğŸ“¥ [LLM] First chunk received - TTFB: ${ttfb}ms" }
                            }
                            emit(frame.text)
                        }
                        is StreamFrame.End -> {
                            val elapsed = Clock.System.now().toEpochMilliseconds() - startTime
                            logger.info { "âœ… [LLM] Stream completed - chunks: $chunkCount, chars: $totalChars, time: ${elapsed}ms" }
                            logger.debug { "StreamFrame.End -> finishReason=${frame.finishReason}, metaInfo=${frame.metaInfo}" }
                            frame.metaInfo?.let { metaInfo ->
                                lastTokenInfo = TokenInfo(
                                    totalTokens = metaInfo.totalTokensCount ?: 0,
                                    inputTokens = metaInfo.inputTokensCount ?: 0,
                                    outputTokens = metaInfo.outputTokensCount ?: 0,
                                    timestamp = Clock.System.now().toEpochMilliseconds()
                                )
                                logger.info { "ğŸ“Š [LLM] Token usage - input: ${lastTokenInfo.inputTokens}, output: ${lastTokenInfo.outputTokens}, total: ${lastTokenInfo.totalTokens}" }

                                onTokenUpdate?.invoke(lastTokenInfo)
                                if (compressionConfig.autoCompressionEnabled) {
                                    val maxTokens = getMaxTokens()
                                    if (lastTokenInfo.needsCompression(maxTokens, compressionConfig.contextPercentageThreshold)) {
                                        onCompressionNeeded?.invoke(lastTokenInfo.inputTokens, maxTokens)
                                    }
                                }
                            }

                            messagesSinceLastCompression++
                        }
                        is StreamFrame.ToolCall -> { /* Tool calls (å¯ä»¥åç»­æ‰©å±•) */ }
                    }
                }
        } catch (e: Exception) {
            val elapsed = Clock.System.now().toEpochMilliseconds() - startTime
            logger.error { "âŒ [LLM] Stream error after ${elapsed}ms - chunks: $chunkCount, error: ${e.message}" }
            throw e
        }
    }

    suspend fun sendPrompt(prompt: String): String {
        return try {
            val agent = AIAgent(promptExecutor = executor, llmModel = model)
            agent.run(prompt)
        } catch (e: Exception) {
            "[Error: ${e.message}]"
        }
    }

    private suspend fun compilePrompt(userPrompt: String, fileSystem: ProjectFileSystem): String {
        val compiledResult = actualCompilerService.compile(userPrompt, fileSystem)

        if (compiledResult.hasError) {
            logger.warn { "âš ï¸ [KoogLLMService] ç¼–è¯‘é”™è¯¯ (${actualCompilerService.getName()}): ${compiledResult.errorMessage}" }
        }

        logger.debug { "ğŸ“ [KoogLLMService] ä½¿ç”¨ç¼–è¯‘å™¨: ${actualCompilerService.getName()}, IDEåŠŸèƒ½: ${actualCompilerService.supportsIdeFeatures()}" }

        return compiledResult.output
    }

    private fun buildPrompt(finalPrompt: String, historyMessages: List<Message>) = prompt(
        id = "chat",
        params = LLMParams(
            temperature = config.temperature,
            toolChoice = LLMParams.ToolChoice.None
        )
    ) {
        historyMessages.forEach { message ->
            when (message.role) {
                MessageRole.USER -> user(message.content)
                MessageRole.ASSISTANT -> assistant(message.content)
                MessageRole.SYSTEM -> system(message.content)
            }
        }

        user(finalPrompt)
    }

    suspend fun validateConfig(): Result<String> {
        return try {
            val response = sendPrompt("Say 'OK' if you can hear me.")
            Result.success(response)
        } catch (e: Exception) {
            Result.failure(e)
        }
    }

    /**
     * å°è¯•å‹ç¼©å†å²æ¶ˆæ¯
     * 
     * @param historyMessages å®Œæ•´çš„å¯¹è¯å†å²
     * @param force æ˜¯å¦å¼ºåˆ¶å‹ç¼©ï¼ˆå¿½ç•¥é˜ˆå€¼å’Œå¤±è´¥è®°å½•ï¼‰
     * @return å‹ç¼©ç»“æœ
     */
    suspend fun tryCompressHistory(
        historyMessages: List<Message>,
        force: Boolean = false
    ): CompressionResult {
        // å¦‚æœä¹‹å‰å‹ç¼©å¤±è´¥ä¸”æ¶ˆæ¯æ•°é‡ä¸è¶³ï¼Œè·³è¿‡
        if (!force && hasFailedCompressionAttempt && 
            messagesSinceLastCompression < compressionConfig.retryAfterMessages) {
            return CompressionResult(
                newMessages = null,
                info = ChatCompressionInfo(
                    originalTokenCount = lastTokenInfo.inputTokens,
                    newTokenCount = lastTokenInfo.inputTokens,
                    compressionStatus = CompressionStatus.NOOP,
                    errorMessage = "ç­‰å¾…æ›´å¤šæ¶ˆæ¯åå†é‡è¯•å‹ç¼©"
                )
            )
        }
        
        val maxTokens = getMaxTokens()
        val result = compressionService.compress(
            messages = historyMessages,
            currentTokenCount = lastTokenInfo.inputTokens,
            maxTokens = maxTokens,
            force = force
        )
        
        // æ›´æ–°çŠ¶æ€
        when (result.info.compressionStatus) {
            CompressionStatus.COMPRESSED -> {
                hasFailedCompressionAttempt = false
                messagesSinceLastCompression = 0
                // æ›´æ–° token ä¿¡æ¯
                lastTokenInfo = lastTokenInfo.copy(
                    inputTokens = result.info.newTokenCount
                )
            }
            CompressionStatus.COMPRESSION_FAILED_INFLATED_TOKEN_COUNT,
            CompressionStatus.COMPRESSION_FAILED_TOKEN_COUNT_ERROR,
            CompressionStatus.COMPRESSION_FAILED_ERROR -> {
                hasFailedCompressionAttempt = !force
                messagesSinceLastCompression = 0
            }
            CompressionStatus.NOOP -> {
                // æ— æ“ä½œ
            }
        }
        
        return result
    }
    
    /**
     * è·å–æœ€åçš„ token ä¿¡æ¯
     */
    fun getLastTokenInfo(): TokenInfo = lastTokenInfo
    
    /**
     * è·å–æ¨¡å‹çš„æœ€å¤§ token æ•°
     */
    fun getMaxTokens(): Int {
        // ä¼˜å…ˆä½¿ç”¨æ¨¡å‹è‡ªå¸¦çš„ maxTokens
        return (model.maxOutputTokens as? Int) ?: config.maxTokens
    }
    
    /**
     * é‡ç½®å‹ç¼©çŠ¶æ€
     */
    fun resetCompressionState() {
        hasFailedCompressionAttempt = false
        messagesSinceLastCompression = 0
    }

    companion object {
        fun create(
            config: ModelConfig, 
            compressionConfig: CompressionConfig = CompressionConfig()
        ): KoogLLMService {
            require(config.isValid()) {
                val requirement = if (config.provider == LLMProviderType.OLLAMA) {
                    "baseUrl and modelName"
                } else {
                    "apiKey and modelName"
                }
                "Invalid model configuration: ${config.provider} requires $requirement"
            }
            return KoogLLMService(config, compressionConfig)
        }
    }
}
